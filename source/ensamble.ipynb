{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae67554a",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df562711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/'\n",
    "\n",
    "resnet_best = 'resnet50_3_tuned1.h5'\n",
    "incept_best = 'inception1.h5'\n",
    "nasnet_best = 'NASNetMobile_1.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae142d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "resnet_model = tf.keras.models.load_model(model_path + resnet_best)\n",
    "incept_model = tf.keras.models.load_model(model_path + incept_best)\n",
    "nasnet_model = tf.keras.models.load_model(model_path + nasnet_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cfe28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_prefix(model, prefix: str, custom_objects=None):\n",
    "    '''Adds a prefix to layers and model name while keeping the pre-trained weights\n",
    "    Arguments:\n",
    "        model: a tf.keras model\n",
    "        prefix: a string that would be added to before each layer name\n",
    "        custom_objects: if your model consists of custom layers you shoud add them pass them as a dictionary. \n",
    "            For more information read the following:\n",
    "            https://keras.io/guides/serialization_and_saving/#custom-objects\n",
    "    Returns:\n",
    "        new_model: a tf.keras model having same weights as the input model.\n",
    "    '''\n",
    "    \n",
    "    config = model.get_config()\n",
    "    old_to_new = {}\n",
    "    new_to_old = {}\n",
    "    \n",
    "    for layer in config['layers']:\n",
    "        new_name = prefix + layer['name']\n",
    "        old_to_new[layer['name']], new_to_old[new_name] = new_name, layer['name']\n",
    "        layer['name'] = new_name\n",
    "        layer['config']['name'] = new_name\n",
    "\n",
    "        if len(layer['inbound_nodes']) > 0:\n",
    "            for in_node in layer['inbound_nodes'][0]:\n",
    "                in_node[0] = old_to_new[in_node[0]]\n",
    "    \n",
    "    for input_layer in config['input_layers']:\n",
    "        input_layer[0] = old_to_new[input_layer[0]]\n",
    "    \n",
    "    for output_layer in config['output_layers']:\n",
    "        output_layer[0] = old_to_new[output_layer[0]]\n",
    "    \n",
    "    config['name'] = prefix + config['name']\n",
    "    new_model = tf.keras.Model().from_config(config, custom_objects)\n",
    "    \n",
    "    for layer in new_model.layers:\n",
    "        layer.set_weights(model.get_layer(new_to_old[layer.name]).get_weights())\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "resnet_model2 = add_prefix(resnet_model, 'resnetmj_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0921127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [resnet_model2, incept_model, nasnet_model]\n",
    "model_input = tf.keras.Input(shape=(224, 224, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
    "ensemble_model = tf.keras.Model(inputs=model_input, outputs=ensemble_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14629e95",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1184a0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17154, 224, 224, 3) (17154,) (5718, 224, 224, 3) (5718,) (5718, 224, 224, 3) (5718,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "folder_path = 'dataset_224_224/'\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = [], [], [], [], [], []\n",
    "\n",
    "with open(folder_path+'x_train.pickle', 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "with open(folder_path+'y_train.pickle', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(folder_path+'x_val.pickle', 'rb') as f:\n",
    "    x_val = pickle.load(f)\n",
    "\n",
    "with open(folder_path+'y_val.pickle', 'rb') as f:\n",
    "    y_val = pickle.load(f)\n",
    "\n",
    "with open(folder_path+'x_test.pickle', 'rb') as f:\n",
    "    x_test = pickle.load(f)\n",
    "\n",
    "with open(folder_path+'y_test.pickle', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "    \n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4e05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17154, 224, 224, 3) (17154, 3) (5718, 224, 224, 3) (5718, 3) (5718, 224, 224, 3) (5718, 3)\n"
     ]
    }
   ],
   "source": [
    "train_images = x_train\n",
    "train_labels = y_train\n",
    "val_images = x_val\n",
    "val_labels = y_val\n",
    "test_images = x_test\n",
    "test_labels = y_test\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(train_labels, 3)\n",
    "y_val = tf.keras.utils.to_categorical(val_labels, 3)\n",
    "y_test = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "y_train.shape, y_val.shape, y_test.shape\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "checkpoint_path = \"/Users/jeonminjeong/Downloads/알파코/프로젝트/이미지인식/checkpoints/ensamble/cp-4-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, # 고유한 파일이름 부여 가능 \n",
    "    monitor='val_loss', # val_loss값을 기준으로 최저점일때를 기록하고 싶다면\n",
    "    save_best_only=True, # True를 주면 monitor기준 최고상태 기록\n",
    "    save_weights_only=False, # False면 모델 레이어 및 weight값 모두\n",
    "    save_freq='epoch' # 체크포인트 저장하는 에폭주기 설정 가능\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0)\n",
    "\n",
    "model_name = '_Ensamble'\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d\") # Y : 연도 / m : 월 / d : 일 / H : 시 / M : 분 / S : 초\n",
    "\n",
    "log_dir = \"logs/\" + current_time + model_name\n",
    "board_ensamble = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) # epoch마다 히스토그램 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d758ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.compile(optimizer = tf.keras.optimizers.Adam( learning_rate= 0.0001),\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_classifi_6 = ensemble_model.fit(train_images, y_train, epochs = 30, validation_data=(val_images, y_val), batch_size=125\n",
    "                            , callbacks=[cp_callback, early, board_ensamble, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_classifi_6.history['loss'])\n",
    "print(hist_classifi_6.history['accuracy'])\n",
    "print(hist_classifi_6.history['val_loss'])\n",
    "print(hist_classifi_6.history['val_accuracy'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist_classifi_6.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist_classifi_6.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(hist_classifi_6.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist_classifi_6.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d39f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:15:08.101265: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 - 201s - loss: 0.5425 - accuracy: 0.8596 - 201s/epoch - 1s/step\n",
      "loss:  0.542522132396698 , acc:  0.859566330909729\n"
     ]
    }
   ],
   "source": [
    "#모델 검증\n",
    "test_loss, test_acc = ensemble_model.evaluate(test_images, y_test, verbose=2)\n",
    "print('loss: ', test_loss, ', acc: ', test_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03764a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.save('models/ensemble.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd959822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ebdde8fc38c1600f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ebdde8fc38c1600f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0104832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 55). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/resnet50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/resnet50/assets\n"
     ]
    }
   ],
   "source": [
    "resnet_model.save('models/resnet50', save_format=\"tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
